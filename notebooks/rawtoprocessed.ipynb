{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326dfe5a",
   "metadata": {},
   "source": [
    "\n",
    "# RAW → PROCESSED Pipeline\n",
    "## This notebook reads the raw banking transaction dataset and produces:\n",
    "### cleaned_transactions.csv\n",
    "### transactions_feature_engineered.csv\n",
    "### customer_rfm_summary.csv\n",
    "### customer_features_for_model.csv\n",
    "### fraud_model_dataset.csv\n",
    "### churn_model_dataset.csv\n",
    "### segmentation_dataset.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c7166",
   "metadata": {},
   "source": [
    "## Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a7b9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67136947",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e11bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW DIR      : /Users/goureshmadye/Documents/DataAnalyst/Integrated-Banking-Insights-Risk-Customer-Analytics/data/raw\n",
      "PROCESSED DIR: /Users/goureshmadye/Documents/DataAnalyst/Integrated-Banking-Insights-Risk-Customer-Analytics/data/processed\n",
      "RAW FILE     : /Users/goureshmadye/Documents/DataAnalyst/Integrated-Banking-Insights-Risk-Customer-Analytics/data/raw/banking_transactions_2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(\"../\")\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Change this if your raw filename is different\n",
    "RAW_FILE = DATA_RAW_DIR / \"banking_transactions_2023_2024.csv\"\n",
    "\n",
    "# Ensure processed dir exists\n",
    "DATA_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"RAW DIR      :\", DATA_RAW_DIR.resolve())\n",
    "print(\"PROCESSED DIR:\", DATA_PROCESSED_DIR.resolve())\n",
    "print(\"RAW FILE     :\", RAW_FILE.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b7415",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da7c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_transactions(raw_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load raw transaction CSV.\"\"\"\n",
    "    df = pd.read_csv(raw_file)\n",
    "    print(f\"Loaded raw data: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_column_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Strip spaces and unify capitalization / underscores for column names.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "        .str.replace(\"/\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert common columns to appropriate data types.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Date columns\n",
    "    possible_date_cols = [\"Transaction_Date\", \"transaction_date\"]\n",
    "    for col in possible_date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Numeric columns (adjust if your dataset has slightly different names)\n",
    "    numeric_cols = [\n",
    "        \"Transaction_Amount\",\n",
    "        \"Customer_Age\",\n",
    "        \"Customer_Income\",\n",
    "        \"Account_Balance\",\n",
    "        \"Discount_Applied\",\n",
    "        \"Loyalty_Points_Earned\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Fraud flag to binary 0/1 if present\n",
    "    if \"Fraud_Flag\" in df.columns:\n",
    "        mapping = {\n",
    "            \"Yes\": 1, \"Y\": 1, \"True\": 1, True: 1,\n",
    "            \"No\": 0, \"N\": 0, \"False\": 0, False: 0\n",
    "        }\n",
    "        df[\"Fraud_Flag\"] = df[\"Fraud_Flag\"].map(mapping).fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing values:\n",
    "    - numeric → median\n",
    "    - categorical → 'Unknown'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        df[col].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def cap_outliers(\n",
    "    df: pd.DataFrame,\n",
    "    cols: List[str],\n",
    "    lower_quantile: float = 0.01,\n",
    "    upper_quantile: float = 0.99,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Cap outliers at specified quantiles for given columns.\n",
    "    Returns:\n",
    "        df_capped, caps_df (caps_df holds the bounds for reference)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    caps = {}\n",
    "\n",
    "    for col in cols:\n",
    "        if col in df.columns:\n",
    "            lower = df[col].quantile(lower_quantile)\n",
    "            upper = df[col].quantile(upper_quantile)\n",
    "            df[col] = df[col].clip(lower=lower, upper=upper)\n",
    "            caps[col] = {\"lower\": lower, \"upper\": upper}\n",
    "\n",
    "    caps_df = pd.DataFrame(caps).T\n",
    "    return df, caps_df\n",
    "\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add time-based features derived from Transaction_Date.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    date_col = \"Transaction_Date\" if \"Transaction_Date\" in df.columns else None\n",
    "    if date_col is None:\n",
    "        print(\"WARNING: Transaction_Date column not found. Skipping time features.\")\n",
    "        return df\n",
    "\n",
    "    df[\"transaction_year\"] = df[date_col].dt.year\n",
    "    df[\"transaction_month\"] = df[date_col].dt.to_period(\"M\").astype(str)\n",
    "    df[\"transaction_day\"] = df[date_col].dt.day\n",
    "    df[\"transaction_hour\"] = df[date_col].dt.hour\n",
    "    df[\"day_of_week\"] = df[date_col].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_account_behavior_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute customer/account-level behavioral features.\n",
    "    These features are aggregated per Account_Number.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Account_Number\" not in df.columns:\n",
    "        print(\"WARNING: Account_Number column not found. Skipping behavior features.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Helper flags\n",
    "    df[\"is_failed\"] = df[\"Transaction_Status\"].eq(\"Failed\") if \"Transaction_Status\" in df.columns else 0\n",
    "    df[\"is_online\"] = df[\"Payment_Method\"].eq(\"Online\") if \"Payment_Method\" in df.columns else 0\n",
    "\n",
    "    group_cols = [\"Account_Number\"]\n",
    "    agg_dict = {\n",
    "        \"Transaction_ID\": \"count\" if \"Transaction_ID\" in df.columns else \"size\",\n",
    "        \"Transaction_Amount\": [\"sum\", \"mean\", \"std\"],\n",
    "        \"is_failed\": \"sum\",\n",
    "        \"is_online\": \"sum\",\n",
    "        \"Discount_Applied\": \"mean\" if \"Discount_Applied\" in df.columns else \"sum\",\n",
    "        \"Loyalty_Points_Earned\": \"sum\" if \"Loyalty_Points_Earned\" in df.columns else \"sum\",\n",
    "    }\n",
    "\n",
    "    # Filter out missing columns in agg_dict\n",
    "    agg_dict_clean = {}\n",
    "    for key, val in agg_dict.items():\n",
    "        if key in df.columns:\n",
    "            agg_dict_clean[key] = val\n",
    "\n",
    "    acc = df.groupby(\"Account_Number\").agg(agg_dict_clean)\n",
    "\n",
    "    # Flatten multiindex columns if any\n",
    "    acc.columns = [\"_\".join([c for c in col if c]) if isinstance(col, tuple) else col for col in acc.columns]\n",
    "\n",
    "    # Rename key metrics\n",
    "    rename_map = {}\n",
    "    for col in acc.columns:\n",
    "        if col.startswith(\"Transaction_ID_\"):\n",
    "            rename_map[col] = \"txn_count\"\n",
    "        if col.startswith(\"Transaction_Amount_sum\"):\n",
    "            rename_map[col] = \"total_spend\"\n",
    "        if col.startswith(\"Transaction_Amount_mean\"):\n",
    "            rename_map[col] = \"avg_spend\"\n",
    "        if col.startswith(\"Transaction_Amount_std\"):\n",
    "            rename_map[col] = \"std_spend\"\n",
    "        if col.startswith(\"is_failed_sum\"):\n",
    "            rename_map[col] = \"failed_txn_count\"\n",
    "        if col.startswith(\"is_online_sum\"):\n",
    "            rename_map[col] = \"online_txn_count\"\n",
    "\n",
    "    acc.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Derive ratios\n",
    "    if \"online_txn_count\" in acc.columns and \"txn_count\" in acc.columns:\n",
    "        acc[\"online_txn_ratio\"] = acc[\"online_txn_count\"] / acc[\"txn_count\"].replace(0, np.nan)\n",
    "\n",
    "    if \"failed_txn_count\" in acc.columns and \"txn_count\" in acc.columns:\n",
    "        acc[\"failed_txn_ratio\"] = acc[\"failed_txn_count\"] / acc[\"txn_count\"].replace(0, np.nan)\n",
    "\n",
    "    acc.reset_index(inplace=True)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def compute_rfm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute RFM metrics per Account_Number:\n",
    "    - Recency (days since last transaction)\n",
    "    - Frequency (transaction count)\n",
    "    - Monetary (total spend)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Account_Number\" not in df.columns:\n",
    "        print(\"WARNING: Account_Number column not found. Skipping RFM.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    date_col = \"Transaction_Date\"\n",
    "    if date_col not in df.columns:\n",
    "        print(\"WARNING: Transaction_Date column not found. Skipping RFM.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    max_date = df[date_col].max()\n",
    "    print(\"Max transaction date (for recency):\", max_date)\n",
    "\n",
    "    rfm = (\n",
    "        df.groupby(\"Account_Number\")\n",
    "        .agg(\n",
    "            last_txn_date=(date_col, \"max\"),\n",
    "            frequency=(\"Transaction_ID\", \"count\") if \"Transaction_ID\" in df.columns else (date_col, \"count\"),\n",
    "            monetary=(\"Transaction_Amount\", \"sum\"),\n",
    "            avg_monetary=(\"Transaction_Amount\", \"mean\"),\n",
    "            customer_age=(\"Customer_Age\", \"max\") if \"Customer_Age\" in df.columns else (date_col, \"size\"),\n",
    "            customer_income=(\"Customer_Income\", \"max\") if \"Customer_Income\" in df.columns else (date_col, \"size\"),\n",
    "            account_balance=(\"Account_Balance\", \"last\") if \"Account_Balance\" in df.columns else (date_col, \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    rfm[\"recency_days\"] = (max_date - rfm[\"last_txn_date\"]).dt.days\n",
    "\n",
    "    return rfm\n",
    "\n",
    "\n",
    "def compute_category_share(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute share of spend by Category per Account_Number.\n",
    "    Returns a wide table with columns like:\n",
    "    - cat_share_Grocery\n",
    "    - cat_share_Entertainment\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Account_Number\" not in df.columns or \"Category\" not in df.columns:\n",
    "        print(\"WARNING: Account_Number or Category column not found. Skipping category share.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    pivot = pd.pivot_table(\n",
    "        df,\n",
    "        index=\"Account_Number\",\n",
    "        columns=\"Category\",\n",
    "        values=\"Transaction_Amount\",\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "\n",
    "    # Convert to share of total spend\n",
    "    totals = pivot.sum(axis=1).replace(0, np.nan)\n",
    "    share = pivot.div(totals, axis=0)\n",
    "\n",
    "    # Prefix column names\n",
    "    share.columns = [f\"cat_share_{str(c)}\" for c in share.columns]\n",
    "    share.reset_index(inplace=True)\n",
    "    return share\n",
    "\n",
    "\n",
    "def build_fraud_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare dataset for fraud modeling (transaction-level).\"\"\"\n",
    "    df = df.copy()\n",
    "    if \"Fraud_Flag\" not in df.columns:\n",
    "        print(\"WARNING: Fraud_Flag not found. Fraud dataset may be empty.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Keep only rows where Fraud_Flag is known (0/1)\n",
    "    fraud_df = df[df[\"Fraud_Flag\"].isin([0, 1])].copy()\n",
    "\n",
    "    # Example selection of useful columns\n",
    "    candidate_cols = [\n",
    "        \"Transaction_ID\",\n",
    "        \"Account_Number\",\n",
    "        \"Transaction_Amount\",\n",
    "        \"Transaction_Type\",\n",
    "        \"Category\",\n",
    "        \"Payment_Method\",\n",
    "        \"Transaction_Status\",\n",
    "        \"Customer_Age\",\n",
    "        \"Customer_Income\",\n",
    "        \"Account_Balance\",\n",
    "        \"transaction_year\",\n",
    "        \"transaction_month\",\n",
    "        \"transaction_day\",\n",
    "        \"transaction_hour\",\n",
    "        \"day_of_week\",\n",
    "        \"is_weekend\",\n",
    "        \"Fraud_Flag\",\n",
    "    ]\n",
    "\n",
    "    cols_existing = [c for c in candidate_cols if c in fraud_df.columns]\n",
    "    fraud_df = fraud_df[cols_existing]\n",
    "\n",
    "    return fraud_df\n",
    "\n",
    "\n",
    "def build_churn_dataset(rfm_df: pd.DataFrame, churn_days: int = 90) -> pd.DataFrame:\n",
    "    \"\"\"Build churn dataset using RFM summary. Churn = no txn in last `churn_days`.\"\"\"\n",
    "    rfm = rfm_df.copy()\n",
    "    if \"recency_days\" not in rfm.columns:\n",
    "        print(\"WARNING: recency_days not in RFM. Cannot build churn dataset correctly.\")\n",
    "        rfm[\"churn_label\"] = 0\n",
    "        return rfm\n",
    "\n",
    "    rfm[\"churn_label\"] = (rfm[\"recency_days\"] > churn_days).astype(int)\n",
    "    return rfm\n",
    "\n",
    "\n",
    "def build_segmentation_dataset(\n",
    "    rfm_df: pd.DataFrame,\n",
    "    cat_share_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build dataset for customer segmentation (RFM + category share + demographics).\n",
    "    \"\"\"\n",
    "    if rfm_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    seg = rfm_df.copy()\n",
    "\n",
    "    if not cat_share_df.empty:\n",
    "        seg = seg.merge(cat_share_df, on=\"Account_Number\", how=\"left\")\n",
    "\n",
    "    # Example columns to keep for clustering\n",
    "    keep_cols = [\n",
    "        \"Account_Number\",\n",
    "        \"recency_days\",\n",
    "        \"frequency\",\n",
    "        \"monetary\",\n",
    "        \"avg_monetary\",\n",
    "        \"customer_age\",\n",
    "        \"customer_income\",\n",
    "        \"account_balance\",\n",
    "    ]\n",
    "    seg_cols_existing = [c for c in keep_cols if c in seg.columns]\n",
    "\n",
    "    # Add any cat_share columns\n",
    "    cat_cols = [c for c in seg.columns if c.startswith(\"cat_share_\")]\n",
    "    seg_cols_existing += cat_cols\n",
    "\n",
    "    seg = seg[seg_cols_existing]\n",
    "\n",
    "    return seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e206a",
   "metadata": {},
   "source": [
    "## Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9309473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    # 1. Load raw\n",
    "    df_raw = load_raw_transactions(RAW_FILE)\n",
    "\n",
    "    # 2. Standardize column names\n",
    "    df = standardize_column_names(df_raw)\n",
    "\n",
    "    # 3. Convert data types\n",
    "    df = convert_dtypes(df)\n",
    "\n",
    "    # 4. Handle missing values\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    # 5. Cap outliers for key numeric columns\n",
    "    outlier_cols = [\"Transaction_Amount\", \"Customer_Income\", \"Account_Balance\"]\n",
    "    df, caps_df = cap_outliers(df, outlier_cols)\n",
    "    print(\"\\nOutlier caps used:\")\n",
    "    print(caps_df)\n",
    "\n",
    "    # Save cleaned base\n",
    "    cleaned_path = DATA_PROCESSED_DIR / \"cleaned_transactions.csv\"\n",
    "    df.to_csv(cleaned_path, index=False)\n",
    "    print(f\"\\nSaved cleaned transactions → {cleaned_path}\")\n",
    "\n",
    "    # 6. Add time-based features\n",
    "    df_feat = add_time_features(df)\n",
    "\n",
    "    # Save feature engineered transaction-level data\n",
    "    feat_path = DATA_PROCESSED_DIR / \"transactions_feature_engineered.csv\"\n",
    "    df_feat.to_csv(feat_path, index=False)\n",
    "    print(f\"Saved feature-engineered transactions → {feat_path}\")\n",
    "\n",
    "    # 7. Compute account-level behavior features\n",
    "    acc_behavior = compute_account_behavior_features(df_feat)\n",
    "    if not acc_behavior.empty:\n",
    "        behavior_path = DATA_PROCESSED_DIR / \"customer_features_for_model.csv\"\n",
    "        acc_behavior.to_csv(behavior_path, index=False)\n",
    "        print(f\"Saved customer behavior features → {behavior_path}\")\n",
    "    else:\n",
    "        print(\"No customer behavior features computed.\")\n",
    "\n",
    "    # 8. Compute RFM metrics\n",
    "    rfm = compute_rfm(df_feat)\n",
    "    if not rfm.empty:\n",
    "        rfm_path = DATA_PROCESSED_DIR / \"customer_rfm_summary.csv\"\n",
    "        rfm.to_csv(rfm_path, index=False)\n",
    "        print(f\"Saved RFM summary → {rfm_path}\")\n",
    "    else:\n",
    "        print(\"RFM summary is empty; check your data.\")\n",
    "\n",
    "    # 9. Compute category share & segmentation dataset\n",
    "    cat_share = compute_category_share(df_feat)\n",
    "    seg = build_segmentation_dataset(rfm, cat_share)\n",
    "    if not seg.empty:\n",
    "        seg_path = DATA_PROCESSED_DIR / \"segmentation_dataset.csv\"\n",
    "        seg.to_csv(seg_path, index=False)\n",
    "        print(f\"Saved segmentation dataset → {seg_path}\")\n",
    "    else:\n",
    "        print(\"Segmentation dataset is empty; check your data.\")\n",
    "\n",
    "    # 10. Build fraud modeling dataset\n",
    "    fraud_df = build_fraud_dataset(df_feat.merge(acc_behavior, on=\"Account_Number\", how=\"left\") if not acc_behavior.empty else df_feat)\n",
    "    if not fraud_df.empty:\n",
    "        fraud_path = DATA_PROCESSED_DIR / \"fraud_model_dataset.csv\"\n",
    "        fraud_df.to_csv(fraud_path, index=False)\n",
    "        print(f\"Saved fraud model dataset → {fraud_path}\")\n",
    "    else:\n",
    "        print(\"Fraud model dataset is empty; check Fraud_Flag column.\")\n",
    "\n",
    "    # 11. Build churn modeling dataset from RFM\n",
    "    churn_df = build_churn_dataset(rfm, churn_days=90)\n",
    "    if not churn_df.empty:\n",
    "        churn_path = DATA_PROCESSED_DIR / \"churn_model_dataset.csv\"\n",
    "        churn_df.to_csv(churn_path, index=False)\n",
    "        print(f\"Saved churn model dataset → {churn_path}\")\n",
    "    else:\n",
    "        print(\"Churn model dataset is empty; check RFM computations.\")\n",
    "\n",
    "    print(\"\\nPipeline completed successfully ✅\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef0e79",
   "metadata": {},
   "source": [
    "## Run the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a5208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data: 5,389 rows, 20 columns\n",
      "\n",
      "Outlier caps used:\n",
      "                         lower        upper\n",
      "Transaction_Amount     51.3780    4958.1344\n",
      "Customer_Income     21273.8512  148589.8880\n",
      "Account_Balance       258.1616   19781.6228\n",
      "\n",
      "Saved cleaned transactions → ../data/processed/cleaned_transactions.csv\n",
      "Saved feature-engineered transactions → ../data/processed/transactions_feature_engineered.csv\n",
      "Saved customer behavior features → ../data/processed/customer_features_for_model.csv\n",
      "Max transaction date (for recency): 2025-01-20 12:21:18\n",
      "Saved RFM summary → ../data/processed/customer_rfm_summary.csv\n",
      "Saved segmentation dataset → ../data/processed/segmentation_dataset.csv\n",
      "Saved fraud model dataset → ../data/processed/fraud_model_dataset.csv\n",
      "Saved churn model dataset → ../data/processed/churn_model_dataset.csv\n",
      "\n",
      "Pipeline completed successfully ✅\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
